
import streamlit as st
import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import classification_report
import json
import time
import random

# --- Configuraci√≥n inicial de la p√°gina de Streamlit ---
# Esta debe ser la PRIMERA llamada a st. en todo el script, despu√©s de los imports.
st.set_page_config(
    page_title="Moderador de Chat IA",
    layout="centered",
    page_icon="ü§ñ",
    initial_sidebar_state="collapsed" 
)

# --- 0. Configuraci√≥n inicial y carga del modelo/tokenizador ---
FINETUNED_MODEL_PATH = './models/modelo_toxicidad_guardado'
BASE_MODEL_NAME_FOR_TOKENIZER = 'distilbert-base-uncased'

num_classes = 4
id_to_label = {
    0: 'Acci√≥n/Juego',
    1: 'Gravemente T√≥xico',
    2: 'Levemente T√≥xico',
    3: 'No T√≥xico'
}

label_to_id = {v: k for k, v in id_to_label.items()}

@st.cache_resource
def load_resources_cached(model_path_cached, base_model_name_for_tokenizer_cached, num_labels_cached):
    """
    Carga el tokenizador y el modelo finetuneado, y lo mueve al dispositivo adecuado.
    Esta funci√≥n se cachea con st.cache_resource para una carga √∫nica.
    """
    try:
        tokenizer = AutoTokenizer.from_pretrained(base_model_name_for_tokenizer_cached)
        model = AutoModelForSequenceClassification.from_pretrained(model_path_cached, num_labels=num_labels_cached)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        model.eval()

        
        return tokenizer, model, device

    except Exception as e:
        st.error(f"¬°Ups! Error al cargar el modelo o tokenizador desde '{model_path_cached}': {e}")
        st.info("Aseg√∫rate de que el modelo est√© entrenado y guardado correctamente en esa ruta.")
        st.info("Si est√°s ejecutando la app por primera vez o has movido archivos, verifica la ruta y los permisos.")
        return None, None, None

# Llama a la funci√≥n cacheada para cargar los recursos
tokenizer, model, device = load_resources_cached(FINETUNED_MODEL_PATH, BASE_MODEL_NAME_FOR_TOKENIZER, num_classes)

if model is None or tokenizer is None:
    st.stop() # Detiene la ejecuci√≥n de la app si el modelo no se pudo cargar

# --- Inicializaci√≥n de variables de estado de Streamlit ---
# Todas las variables de sesi√≥n deben inicializarse para evitar errores.
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'grave_count' not in st.session_state:
    st.session_state.grave_count = 0
if 'leve_count' not in st.session_state:
    st.session_state.leve_count = 0
if 'is_banned' not in st.session_state:
    st.session_state.is_banned = False
if 'chat_input' not in st.session_state: # Para el widget de texto del chat
    st.session_state.chat_input = ""

# Umbrales configurables por la empresa para la Demostraci√≥n Interactiva
if 'max_graves' not in st.session_state:
    st.session_state.max_graves = 3 # Valor por defecto
if 'max_leves' not in st.session_state:
    st.session_state.max_leves = 5 # Valor por defecto
if 'ban_threshold_graves' not in st.session_state:
    st.session_state.ban_threshold_graves = 10 # Umbral de baneo por graves
if 'ban_threshold_total_toxic' not in st.session_state:
    st.session_state.ban_threshold_total_toxic = 15 # Umbral de baneo por toxicidad total

# --- Funciones Auxiliares ---
def get_prediction(text, tokenizer, model, device, id_to_label):
    if model is None or tokenizer is None:
        return "Modelo no cargado", None, None

    inputs = tokenizer(
        text,
        truncation=True,
        padding='max_length',
        max_length=256,
        return_tensors='pt'
    ).to(device)

    with torch.no_grad():
        outputs = model(**inputs)

    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].cpu().numpy()
    predicted_id = np.argmax(probabilities).item()
    predicted_label = id_to_label[predicted_id]

    return predicted_label, probabilities, predicted_id

# Funci√≥n para resetear la demo (usada en la Demostraci√≥n Interactiva)
def reset_demo():
    st.session_state.grave_count = 0
    st.session_state.leve_count = 0
    # Aseguramos que tambi√©n se resetean los contadores de aliados/enemigos
    st.session_state.grave_count_ally = 0
    st.session_state.leve_count_ally = 0
    st.session_state.grave_count_enemy = 0
    st.session_state.leve_count_enemy = 0
    st.session_state.is_banned = False
    st.session_state.chat_history = []
    st.session_state.chat_input = "" 

# Funci√≥n para crear burbujas de mensaje HTML
def create_bubble(message, sender="t√∫", msg_type="normal"):
    colors = {
        "grave": "#ffcccc", # Rojo claro
        "leve": "#ffe0b2",  # Naranja claro
        "no_toxic": "#e0f7fa", # Azul muy claro
        "action": "#e8f5e9", # Verde claro
        "system": "#eeeeee", # Gris muy claro
        "enemy": "#f3e5f5",  # Morado claro
        "ally": "#dcedc8",   # Verde pistacho claro
        "warning": "#fff3cd", # Amarillo claro
        "ban": "#f8d7da",    # Rojo muy claro, m√°s pastel
    }
    style = f"background-color:{colors.get(msg_type, '#f0f0f0')};border-radius:10px;padding:10px;margin:6px 0"
    return f"""
    <div style='{style}'>
        <strong>{sender}:</strong> {message}
    </div>
    """

# --- NAVEGACI√ìN CON PESTA√ëAS (TABS) ---
tab_intro, tab_demo, tab_csv, tab_performance, tab_conclusions = st.tabs([
    "1. Introducci√≥n",
    "2. Demostraci√≥n Interactiva",
    "3. Herramienta de An√°lisis",
    "4. Rendimiento ",
    "5. Pr√≥ximos Pasos"
])

# --- Contenido de las P√°ginas ---

with tab_intro:
    st.title("üõ°Ô∏è Moderador de Chat Inteligente")
    st.subheader("Tu Aliado en la Moderaci√≥n Online")
    st.markdown("---")

    with st.container():
        st.header("üéØ ¬øPor qu√© un Moderador de Chat con IA?")
        st.markdown("""
        La toxicidad en los entornos online, especialmente en videojuegos multijugador, se ha convertido en un problema creciente que afecta tanto a los usuarios como a las plataformas que los albergan.

        Seg√∫n recientes investigaciones, m√°s del **80%** de los jugadores han experimentado alg√∫n tipo de acoso en juegos en l√≠nea. De estos, un **68%** ha recibido amenazas f√≠sicas, acecho o acoso sostenido, mientras que el **53%** ha sido v√≠ctima de ataques basados en raza, g√©nero, orientaci√≥n sexual u otros aspectos personales.

        Este tipo de comportamiento no solo deteriora la experiencia del usuario, sino que tambi√©n genera consecuencias serias a nivel social, incluyendo aislamiento, ansiedad e incluso pensamientos depresivos en los afectados.

        En este contexto, un Moderador de Chat Inteligente basado en IA no solo es una herramienta para detectar y filtrar mensajes t√≥xicos, sino una soluci√≥n clave para proteger a las comunidades digitales y garantizar un entorno seguro y saludable para todos.
        """)

    st.markdown("---")

    with st.container():
        st.header("üè¢ ¬øPor qu√© esto importa a las empresas?")
        st.markdown("""
        La toxicidad online no se limita a los videojuegos. Plataformas con interacci√≥n constante ‚Äî foros, redes sociales, e-commerce o servicios de atenci√≥n al cliente ‚Äî enfrentan el desaf√≠o de mantener ambientes positivos y seguros.

        Incorporar un moderador de chat con IA ofrece m√∫ltiples ventajas comerciales:

        - **Mejora la experiencia del usuario**: Los entornos saludables incrementan la satisfacci√≥n, retenci√≥n y engagement.
        - **Protecci√≥n de la marca**: Evita que contenidos da√±inos o pol√©micos afecten la reputaci√≥n de la empresa.
        - **Escalabilidad y eficiencia**: La IA modera grandes vol√∫menes de mensajes en tiempo real, sin fatiga ni sesgos.
        - **Reducci√≥n de costos operativos**: Disminuye la dependencia de equipos humanos para tareas repetitivas.
        - **An√°lisis y prevenci√≥n**: Detecta patrones de comportamiento t√≥xico para anticipar y mitigar problemas futuros.
        """)

    st.markdown("---")

    with st.container():
        st.subheader("üìö Fuentes y estudios relevantes")
        with st.expander("üîé Consulta las investigaciones que respaldan estos datos"):
            st.markdown("""
            - üìà *El Tel√©grafo (2024)* reporta que el acoso en juegos online aument√≥ un **74%** en el √∫ltimo a√±o. M√°s del **80%** de jugadores han sufrido acoso, y el **68%** recibi√≥ amenazas f√≠sicas, acecho o acoso sostenido.  
            [Ver art√≠culo](https://www.eltelegrafo.com.ec/noticias/sociedad/6/acoso-juegos-online-aumento-ultimo-ano)

            - üìä *Pew Research Center (2021)* se√±ala que el **41%** de los adultos en EE.UU. ha sido acosado online, un problema transversal en m√∫ltiples plataformas.  
            [Ver estudio](https://www.pewresearch.org/internet/2021/01/13/the-state-of-online-harassment/)
            """)

with tab_demo:

    with st.sidebar:
        st.header("‚öôÔ∏è Ajustes de umbrales")

        # Aseg√∫rate de que los sliders usan las variables de st.session_state para sus valores
        st.session_state.max_graves = st.slider(
            "üî¥ Umbral de advertencia grave", 1, 10,
            value=st.session_state.max_graves, key="slider_max_graves" # A√±adido key
        )
        st.session_state.max_leves = st.slider(
            "üü† Umbral de advertencia leve", 1, 10,
            value=st.session_state.max_leves, key="slider_max_leves" # A√±adido key
        )
        st.session_state.ban_threshold_graves = st.slider(
            "üö´ Umbral para ban por graves", 1, 10,
            value=st.session_state.ban_threshold_graves, key="slider_ban_graves" # A√±adido key
        )
        st.session_state.ban_threshold_total_toxic = st.slider(
            "üö´ Umbral para ban por total t√≥xicos", 1, 20,
            value=st.session_state.ban_threshold_total_toxic, key="slider_ban_total" # A√±adido key
        )

        if st.button("Reiniciar Demo", key="reset_demo_button_sidebar"):
            reset_demo()
           

    st.title("ü§ñ Demo Interactivo: Moderador de Chat")
    st.markdown("---")

    st.header("ü§î ¬øC√≥mo funciona este demo?")
    st.markdown(f"""
    1.  **Escribe un mensaje** en la caja de texto.
    2.  Haz clic en **\"Enviar mensaje\"** o presiona `Enter`.
    3.  El moderador clasificar√° el mensaje en una de las **{num_classes} categor√≠as** ({', '.join(id_to_label.values())}).
    4.  Si se detecta toxicidad, ver√°s una **advertencia**.
    5.  Los mensajes se acumulan como **'T√≥xicos Graves'** y **'T√≥xicos Leves'**. Si superas los umbrales configurados en la barra lateral (actualmente **{st.session_state.max_graves} t√≥xicos graves** y **{st.session_state.max_leves} t√≥xicos leves**, o **{st.session_state.ban_threshold_graves} t√≥xicos graves** o **{st.session_state.ban_threshold_total_toxic} mensajes t√≥xicos totales** para ban), se bloquear√° el chat.
    6.  Usa el bot√≥n **\"Reiniciar Demo\"** en la barra lateral para empezar de nuevo.
    """)

    st.markdown("---")

    st.subheader("Simulador de chat")

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Tus mensajes t√≥xicos graves", st.session_state.grave_count)
        st.metric("Tus mensajes t√≥xicos leves", st.session_state.leve_count)
    with col2:
        # Asegurarse de que estas variables est√©n inicializadas en reset_demo o al inicio del script
        if 'grave_count_ally' not in st.session_state: st.session_state.grave_count_ally = 0
        if 'leve_count_ally' not in st.session_state: st.session_state.leve_count_ally = 0
        st.metric("Aliado - t√≥xicos graves", st.session_state.grave_count_ally)
        st.metric("Aliado - t√≥xicos leves", st.session_state.leve_count_ally)
    with col3:
        # Asegurarse de que estas variables est√©n inicializadas en reset_demo o al inicio del script
        if 'grave_count_enemy' not in st.session_state: st.session_state.grave_count_enemy = 0
        if 'leve_count_enemy' not in st.session_state: st.session_state.leve_count_enemy = 0
        st.metric("Enemigo - t√≥xicos graves", st.session_state.grave_count_enemy)
        st.metric("Enemigo - t√≥xicos leves", st.session_state.leve_count_enemy)

    # --- L√≥gica de baneo y chat ---
    # La visualizaci√≥n del historial de chat se mueve aqu√≠, antes del input,
    # para que siempre sea visible, incluso si el usuario est√° baneado.
    st.markdown("### Historial del Chat:")
    # Mostrar el chat hist√≥rico (en orden cronol√≥gico si se prefiere)
    # Si quieres el m√°s nuevo arriba, usa reversed(st.session_state.chat_history)
    for bubble in st.session_state.chat_history:
        st.markdown(bubble, unsafe_allow_html=True)

    if st.session_state.is_banned:
        st.markdown(create_bubble("üö´ ¬°HAS SIDO BLOQUEADO DEL CHAT! üö´ Excediste los l√≠mites de toxicidad.", "üõë Sistema", "ban"), unsafe_allow_html=True)
        if st.button("Desbloquear (Reiniciar Demo)", key="unban_button"):
            reset_demo()
            # st.experimental_rerun() # No es necesario aqu√≠

    else:
        # --- Funci√≥n Callback para el env√≠o de mensaje ---
        def send_message_and_update_demo():
            user_message = st.session_state.chat_input # Accede al valor del input con su key

            if not user_message: # Si el mensaje est√° vac√≠o
                st.warning("Por favor, escribe un mensaje.")
                return # Salir de la funci√≥n si no hay mensaje

            # Realizar predicci√≥n: ¬°Aqu√≠ es donde se usa el modelo!
            predicted_label, probabilities, predicted_id = get_prediction(user_message, tokenizer, model, device, id_to_label)

            # --- A√±adir mensaje del usuario al historial ---
            if predicted_label == id_to_label[1]:  # Gravemente T√≥xico
                st.session_state.grave_count += 1
                bubble = create_bubble(user_message + f"<br><em>Clasificado como: <b>{predicted_label}</b></em>", "üßë T√∫", "grave")
                st.session_state.chat_history.append(bubble)
                if st.session_state.grave_count >= st.session_state.max_graves:
                    warning = create_bubble("üö® ADVERTENCIA: Demasiados mensajes t√≥xicos graves.", "‚ö†Ô∏è Sistema", "warning")
                    st.session_state.chat_history.append(warning)
                if st.session_state.grave_count >= st.session_state.ban_threshold_graves:
                    st.session_state.is_banned = True
                    ban_msg = create_bubble("üö´ Has sido bloqueado por toxicidad grave.", "üõë Sistema", "ban")
                    st.session_state.chat_history.append(ban_msg)

            elif predicted_label == id_to_label[2]:  # Levemente T√≥xico
                st.session_state.leve_count += 1
                bubble = create_bubble(user_message + f"<br><em>Clasificado como: <b>{predicted_label}</b></em>", "üßë T√∫", "leve")
                st.session_state.chat_history.append(bubble)
                if st.session_state.leve_count >= st.session_state.max_leves:
                    warning = create_bubble("‚ö†Ô∏è AVISO: Varios mensajes t√≥xicos leves.", "‚ö†Ô∏è Sistema", "warning")
                    st.session_state.chat_history.append(warning)
                if (st.session_state.grave_count + st.session_state.leve_count) >= st.session_state.ban_threshold_total_toxic:
                    st.session_state.is_banned = True
                    ban_msg = create_bubble("üö´ Has sido bloqueado por toxicidad acumulada.", "üõë Sistema", "ban")
                    st.session_state.chat_history.append(ban_msg)

            elif predicted_label == id_to_label[3]:  # No T√≥xico
                bubble = create_bubble(user_message + f"<br><em>Clasificado como: <b>{predicted_label}</b></em>", "üßë T√∫", "no_toxic")
                st.session_state.chat_history.append(bubble)

            else: # Acci√≥n/Juego
                bubble = create_bubble(user_message + f"<br><em>Clasificado como: <b>{predicted_label}</b></em>", "üßë T√∫", "action")
                st.session_state.chat_history.append(bubble)

            # --- Mensajes aleatorios de Aliado/Enemigo ---
            def ally_message_generator(): # Renombrado para evitar conflicto con la funci√≥n ally_message() anterior
                options = [
                    ("¬°Buen trabajo en la l√≠nea!", "no_toxic"),
                    ("¬°Reagrup√©monos en mid!", "action"),
                    ("Cuidado con el jungla enemigo...", "leve"),
                    ("¬°Necesito ayuda, me est√°n aplastando!", "grave"),
                ]
                text, msg_type = random.choice(options)
                if msg_type == "grave":
                    st.session_state.grave_count_ally += 1
                elif msg_type == "leve":
                    st.session_state.leve_count_ally += 1
                return create_bubble(text, "üü¢ Aliado", "ally") # Usar "ally" para el tipo de burbuja

            def enemy_message_generator(): # Renombrado
                options = [
                    ("¬øQu√© haces, in√∫til?", "leve"),
                    ("Te voy a aplastar", "grave"),
                    ("Vamos a ganar esta partida.", "no_toxic"),
                    ("C√°llate y farmea.", "action"),
                ]
                text, msg_type = random.choice(options)
                if msg_type == "grave":
                    st.session_state.grave_count_enemy += 1
                elif msg_type == "leve":
                    st.session_state.leve_count_enemy += 1
                return create_bubble(text, "üî¥ Enemigo", "enemy") # Usar "enemy" para el tipo de burbuja

            # A√±adir mensajes de IA al historial
            st.session_state.chat_history.append(ally_message_generator())
            st.session_state.chat_history.append(enemy_message_generator())

            # Vaciar el input de texto despu√©s de todo el procesamiento
            st.session_state.chat_input = ""
            # No necesitas st.experimental_rerun() aqu√≠, on_click maneja el refresco.


        # El campo de entrada de texto
        # Ya no usamos on_change para limpiar, se limpia dentro del callback
        st.text_input("Escribe tu mensaje aqu√≠:", key="chat_input")

        # El bot√≥n que activa la funci√≥n callback para enviar y procesar el mensaje
        st.button("Enviar mensaje", on_click=send_message_and_update_demo, key="send_message_button_demo")

    # Mover la visualizaci√≥n del historial de chat arriba para que siempre sea visible
    # st.markdown("### Historial del Chat:") # Ya lo puse arriba

with tab_csv:
    st.title("üõ†Ô∏è Herramienta de An√°lisis de Toxicidad por Lotes")
    st.markdown("---")

    st.header("Carga tu CSV de Comentarios")
    st.write("""
    Esta herramienta te permite subir un archivo CSV con una columna de comentarios
    y el modelo de IA a√±adir√° una nueva columna clasificando cada comentario
    en 'Gravemente T√≥xico', 'Levemente T√≥xico', 'No T√≥xico' o 'Acci√≥n/Juego'.
    """)

    uploaded_file = st.file_uploader("Sube tu archivo CSV aqu√≠", type="csv")

    if uploaded_file is not None:
        try:
            df_upload = pd.read_csv(uploaded_file)
            st.success("Archivo CSV cargado exitosamente.")
            st.dataframe(df_upload.head())

            # Permite al usuario seleccionar la columna si no es obvio
            column_to_analyze = st.selectbox(
                "Selecciona la columna que contiene los comentarios:",
                df_upload.columns,
                key="csv_column_selector" # A√±adido key
            )

            if st.button("Analizar Comentarios", key="analyze_csv_button"): # A√±adido key
                if model is not None and tokenizer is not None:
                    with st.spinner("Analizando comentarios... Esto puede tardar unos minutos para archivos grandes."):
                        predictions = []
                        # Asegurarse de que los comentarios son strings y manejar NaN/vac√≠os
                        for comment in df_upload[column_to_analyze].astype(str).fillna(''):
                            predicted_label, _, _ = get_prediction(comment, tokenizer, model, device, id_to_label)
                            predictions.append(predicted_label)

                        df_upload['clasificacion_toxicidad'] = predictions
                        st.subheader("Resultados del An√°lisis:")
                        st.dataframe(df_upload)

                        # Opci√≥n de descarga
                        csv_output = df_upload.to_csv(index=False).encode('utf-8')
                        st.download_button(
                            label="Descargar CSV con Clasificaciones",
                            data=csv_output,
                            file_name="comentarios_clasificados.csv",
                            mime="text/csv",
                            key="download_classified_csv" # A√±adido key
                        )
                else:
                    st.error("El modelo no est√° cargado. No se puede realizar el an√°lisis.")
        except Exception as e:
            st.error(f"Error al leer o procesar el archivo CSV: {e}")
            st.info("Aseg√∫rate de que el archivo es un CSV v√°lido y que no contiene caracteres problem√°ticos.")


with tab_performance:
    st.title("üìà Rendimiento del Modelo: Fiabilidad y Capacidades")
    st.markdown("---")

    st.header("¬øHasta qu√© punto es fiable nuestro moderador IA?")
    st.write("""
    Nuestro modelo de Inteligencia Artificial es el coraz√≥n de este sistema de moderaci√≥n. Ha sido entrenado
    con mucho cuidado y probado a fondo para asegurar que es **eficaz y fiable**. Aqu√≠ te explicamos, de
    forma sencilla, qu√© tan bien funciona y d√≥nde brilla m√°s.
    """)

    # --- DATOS DE RENDIMIENTO HARDCODEADOS DIRECTAMENTE EN EL C√ìDIGO ---
    # Estos son los datos que antes se cargaban del JSON. Ahora est√°n aqu√≠.
    hardcoded_report_data = {
        "Acci√≥n/Juego": {"precision": 0.739, "recall": 0.845, "f1-score": 0.788, "support": "N/A"}, # 'support' es el n√∫mero de ejemplos
        "Gravemente T√≥xico": {"precision": 0.872, "recall": 0.886, "f1-score": 0.879, "support": "N/A"},
        "Levemente T√≥xico": {"precision": 0.793, "recall": 0.718, "f1-score": 0.753, "support": "N/A"},
        "No T√≥xico": {"precision": 0.954, "recall": 0.954, "f1-score": 0.954, "support": "N/A"},
        "accuracy": 0.9174,
        "macro avg": {"precision": 0.8395, "recall": 0.85075, "f1-score": 0.8435, "support": "N/A"},
        "weighted avg": {"precision": 0.9175, "recall": 0.9174, "f1-score": 0.9175, "support": "N/A"}
    }

    # Creamos el DataFrame directamente desde los datos hardcodeados
    report_df = pd.DataFrame(hardcoded_report_data).transpose()

    # Redondeamos las columnas num√©ricas para una mejor visualizaci√≥n
    # y manejamos el caso de 'accuracy' que no es un diccionario
    for col in ['precision', 'recall', 'f1-score']:
        if col in report_df.columns:
            report_df[col] = pd.to_numeric(report_df[col], errors='coerce').round(3)
    if 'accuracy' in report_df.index:
        report_df.loc['accuracy'] = pd.to_numeric(report_df.loc['accuracy'], errors='coerce').round(4) # Accuracy puede tener m√°s decimales


    st.subheader("M√©tricas de Evaluaci√≥n Detalladas:")
    st.dataframe(report_df)

    st.markdown("""
    ### **¬øC√≥mo se comporta el modelo? Un resumen para todos:**

    Imagina que el modelo es como un **experto revisor de chat**. Analiza los mensajes y decide
    si son t√≥xicos o no. Basado en nuestras pruebas, aqu√≠ te contamos lo que hace genial y d√≥nde puede mejorar:

    ---

    #### **‚úÖ Lo que el modelo hace EXCEPCIONALMENTE bien (Puntos Fuertes):**

    * **Detecta la toxicidad m√°s grave (¬°Y muy bien!):** Cuando un mensaje es **"Gravemente T√≥xico"**, nuestro modelo es **muy, muy bueno** identific√°ndolo. Tiene una fiabilidad de casi el **88%** en esta tarea. Esto es crucial porque significa que los mensajes m√°s da√±inos y problem√°ticos son r√°pidamente detectados y gestionados. Protege a tu comunidad de lo peor.

    * **Reconoce lo que NO es t√≥xico (¬°Casi perfecto!):** El modelo es **excelente** a la hora de decir cu√°ndo un mensaje es **"No T√≥xico"**. Con una fiabilidad del **95.4%**, casi nunca se equivoca al dejar pasar conversaciones normales y saludables. Esto es vital para no molestar a los usuarios que se comunican bien.

    * **Precisi√≥n general impresionante:** En total, el modelo acierta en casi **9 de cada 10 mensajes** (un **91.74%** de precisi√≥n general). Esto nos dice que, en la mayor√≠a de los casos, puedes confiar en su criterio para mantener el chat limpio.

    ---

    #### **‚ö†Ô∏è D√≥nde el modelo puede ser m√°s "cauteloso" (√Åreas de Oportunidad):**

    * **Toxicidad leve (el matiz es complicado):** Identificar la toxicidad **"Levemente T√≥xica"** es el reto m√°s grande. Piensa en el sarcasmo, las indirectas o los comentarios un poco molestos pero no tan agresivos. Aqu√≠ el modelo es **razonablemente bueno** (alrededor del **75.3%**), pero a veces puede **dejar pasar** algunos mensajes sutiles. Esto es normal en la IA, ya que la l√≠nea entre un comentario "regular" y uno "ligeramente t√≥xico" puede ser muy fina y subjetiva para una m√°quina.

    * **Mensajes de "Acci√≥n/Juego":** Aunque clasifica bien la mayor√≠a, en los mensajes de **"Acci√≥n/Juego"** (como "¬°Vamos a por el objetivo!"), su precisi√≥n es un poco m√°s baja (alrededor del **78.8%**). A veces, puede confundir estos mensajes con otros "No T√≥xicos". Esto se debe a que el lenguaje del juego puede ser muy similar al de las conversaciones normales.

    ---

    #### **En resumen:**

    Este moderador es una **herramienta poderosa y muy fiable** para detectar los mensajes m√°s peligrosos y para asegurar que el chat normal fluye sin problemas. Para las toxicidades m√°s sutiles o las categor√≠as de contenido espec√≠ficas, es una base excelente que puede mejorarse con el tiempo y el uso.

    ---
    """)

    # --- La secci√≥n de descarga del PDF se mantiene igual ---
    st.markdown("---")
    st.header("¬øQuieres ver los n√∫meros exactos en detalle?")
    st.write("""
    Para aquellos interesados en los detalles t√©cnicos, las m√©tricas completas y un an√°lisis exhaustivo
    del rendimiento de nuestro modelo, puedes descargar el informe completo de evaluaci√≥n aqu√≠.
    Contiene todos los datos que los expertos necesitan:
    """)

    try:
        # Aseg√∫rate de que 'informe_modelo.pdf' est√© en la ruta './reports/'
        with open("./reports/informe_modelo.pdf", "rb") as file:
            st.download_button(
                label="Descargar Informe Completo (PDF)",
                data=file,
                file_name="informe_moderador_ia.pdf",
                mime="application/pdf",
                key="download_report_pdf"
            )
    except FileNotFoundError:
        st.warning("El archivo del informe (`informe_modelo.pdf`) no se encontr√≥ en la ruta `./reports/`. Por favor, verifica que la carpeta `reports` y el archivo existan.")
    except Exception as e:
        st.error(f"Error al preparar el archivo para descarga: {e}")


with tab_conclusions:
    st.title("üîÆ Conclusiones y Pr√≥ximos Pasos: El Futuro de la Moderaci√≥n")
    st.markdown("---")

    st.header("Explorando el Horizonte de la Moderaci√≥n Inteligente")
    st.write("""
    Este moderador de chat es un componente fundamental para una estrategia de moderaci√≥n de contenido robusta.
    Mirando hacia el futuro, se pueden explorar varias v√≠as de mejora e integraci√≥n:
    """)
    st.markdown("""
    * **Arquitectura de Integraci√≥n (APIs):** El modelo entrenado puede ser desplegado como un **microservicio** al que las plataformas de chat se conectar√≠an a trav√©s de una API REST (por ejemplo, con Flask, FastAPI). Esto permite una **moderaci√≥n en tiempo real** donde cada mensaje enviado pasa por la IA antes de ser visible.
    * **IA Conversacional para Intervenci√≥n:** Imagina una IA que no solo detecta, sino que tambi√©n interviene.
        * **Feedback Personalizado:** "Lo que has dicho es ofensivo. Por favor, modera tu lenguaje."
        * **Filtrado Activo con Reescritura (IA de Traducci√≥n de Toxicidad):** Esto es un concepto innovador. Un usuario escribe: "*me cago en tu ... no sabes jugar tendrias que hacer esto manco*". El sistema detecta la toxicidad y, antes de que el mensaje sea enviado al chat p√∫blico, lo reescribe a algo como: "*Podr√≠as intentar esto en su lugar. Me estoy frustrando con el juego.*" Esto permite a los usuarios expresar sus emociones, pero asegura que el mensaje que llega al chat sea constructivo y no t√≥xico, manteniendo un flujo de comunicaci√≥n positivo. Es un filtro proactivo que "traduce" la toxicidad en constructividad.

    Este proyecto representa un punto de partida poderoso. La evoluci√≥n hacia sistemas de IA m√°s contextuales y proactivos es el camino hacia comunidades digitales verdaderamente seguras y acogedoras.
    """)