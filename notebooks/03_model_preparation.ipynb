{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc12cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rocio\\OneDrive\\Escritorio\\Bootcamp_DA\\Analyst data\\Proyectos\\moba-chat-moderator\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --- Librerías ---\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from transformers.trainer_utils import IntervalStrategy\n",
    "import torch\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_dir = os.path.abspath('.')\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "sys.path.append(project_root)\n",
    "from src.utils import compute_metrics\n",
    "# --- Cargar DF ---\n",
    "df = pd.read_csv(\"../data/processed/df_final_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1002e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dividir el dataset en conjuntos de entrenamiento, validación y prueba ---\n",
    "# --- IMPORTANTE: Aseguramos que la columna 'text_cleaned' no tenga NaN y sea tipo string ---\n",
    "# Esto es crucial antes de la tokenización para evitar el TypeError\n",
    "df['text_cleaned'] = df['text_cleaned'].fillna('').astype(str)\n",
    "\n",
    "X = df['text_cleaned']\n",
    "y = df['binary_label']\n",
    "\n",
    "# Una buena práctica es dividir primero en entrenamiento+validación y luego el de prueba.\n",
    "# Luego, el conjunto de entrenamiento+validación se divide nuevamente en entrenamiento y validación.\n",
    "\n",
    "# Dividir en Entrenamiento+Validación (80%) y Prueba (20%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Dividir el conjunto Entrenamiento+Validación en Entrenamiento (80% del 80% = 64%) y Validación (20% del 80% = 16%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c0d2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Carga y Preparación del Tokenizer de Hugging Face ---\n",
    "# Vamos a usar un modelo popular y eficiente, como DistilBERT.\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c37c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "train_encodings = tokenizer(\n",
    "    list(X_train),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_length,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "val_encodings = tokenizer(\n",
    "    list(X_val),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_length,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    list(X_test),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_length,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# --- CONVERTIR ETIQUETAS Y DATOS A PyTorch Dataset ---\n",
    "# Esta clase es estándar cuando se usa el Trainer.\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        # Las etiquetas deben ser tensores de PyTorch.\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Aseguramos que los valores de encodings sean también tensores de PyTorch\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Crear los objetos Dataset para entrenamiento, validación y prueba\n",
    "train_dataset = CustomDataset(train_encodings, y_train)\n",
    "val_dataset = CustomDataset(val_encodings, y_val)\n",
    "test_dataset = CustomDataset(test_encodings, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21b4bfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Cargar el modelo pre-entrenado DistilBERT para clasificación ---\n",
    "num_labels = len(df['binary_label'].unique())\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "# Mover el modelo a la GPU si está disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "#Lo usarás para acelerar drásticamente el entrenamiento de tu modelo \n",
    "#evitar errores de memoria agotada con datasets y modelos grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc58076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7210' max='7210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7210/7210 40:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Overall</th>\n",
       "      <th>Recall Overall</th>\n",
       "      <th>Precision Overall</th>\n",
       "      <th>F1 Class 0 No Toxic</th>\n",
       "      <th>Recall Class 0 No Toxic</th>\n",
       "      <th>Precision Class 0 No Toxic</th>\n",
       "      <th>F1 Class 1 Toxic</th>\n",
       "      <th>Recall Class 1 Toxic</th>\n",
       "      <th>Precision Class 1 Toxic</th>\n",
       "      <th>Classification Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.180700</td>\n",
       "      <td>0.166419</td>\n",
       "      <td>0.946424</td>\n",
       "      <td>0.945354</td>\n",
       "      <td>0.946424</td>\n",
       "      <td>0.945884</td>\n",
       "      <td>0.965735</td>\n",
       "      <td>0.980909</td>\n",
       "      <td>0.951024</td>\n",
       "      <td>0.877235</td>\n",
       "      <td>0.831169</td>\n",
       "      <td>0.928707</td>\n",
       "      <td>{'0': {'precision': 0.951023751023751, 'recall': 0.9809089373204933, 'f1-score': 0.9657351962741184, 'support': 5919.0}, '1': {'precision': 0.9287066246056782, 'recall': 0.8311688311688312, 'f1-score': 0.8772348033373063, 'support': 1771.0}, 'accuracy': 0.9464239271781535, 'macro avg': {'precision': 0.9398651878147146, 'recall': 0.9060388842446623, 'f1-score': 0.9214849998057124, 'support': 7690.0}, 'weighted avg': {'precision': 0.9458841371243483, 'recall': 0.9464239271781535, 'f1-score': 0.945353636340296, 'support': 7690.0}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>0.166744</td>\n",
       "      <td>0.949415</td>\n",
       "      <td>0.948854</td>\n",
       "      <td>0.949415</td>\n",
       "      <td>0.948808</td>\n",
       "      <td>0.967434</td>\n",
       "      <td>0.976178</td>\n",
       "      <td>0.958845</td>\n",
       "      <td>0.886754</td>\n",
       "      <td>0.859966</td>\n",
       "      <td>0.915264</td>\n",
       "      <td>{'0': {'precision': 0.9588450049784268, 'recall': 0.9761784085149519, 'f1-score': 0.9674340728338217, 'support': 5919.0}, '1': {'precision': 0.9152644230769231, 'recall': 0.859966120835686, 'f1-score': 0.8867540029112082, 'support': 1771.0}, 'accuracy': 0.9494148244473342, 'macro avg': {'precision': 0.937054714027675, 'recall': 0.918072264675319, 'f1-score': 0.927094037872515, 'support': 7690.0}, 'weighted avg': {'precision': 0.9488084366367412, 'recall': 0.9494148244473342, 'f1-score': 0.9488535261715397, 'support': 7690.0}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.096600</td>\n",
       "      <td>0.194868</td>\n",
       "      <td>0.947204</td>\n",
       "      <td>0.947183</td>\n",
       "      <td>0.947204</td>\n",
       "      <td>0.947163</td>\n",
       "      <td>0.965715</td>\n",
       "      <td>0.966042</td>\n",
       "      <td>0.965389</td>\n",
       "      <td>0.885246</td>\n",
       "      <td>0.884246</td>\n",
       "      <td>0.886248</td>\n",
       "      <td>{'0': {'precision': 0.9653891608981935, 'recall': 0.9660415610745058, 'f1-score': 0.9657152508022293, 'support': 5919.0}, '1': {'precision': 0.8862478777589134, 'recall': 0.8842461885940147, 'f1-score': 0.8852459016393442, 'support': 1771.0}, 'accuracy': 0.9472041612483745, 'macro avg': {'precision': 0.9258185193285535, 'recall': 0.9251438748342602, 'f1-score': 0.9254805762207867, 'support': 7690.0}, 'weighted avg': {'precision': 0.947162995431397, 'recall': 0.9472041612483745, 'f1-score': 0.9471832329390993, 'support': 7690.0}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>0.245786</td>\n",
       "      <td>0.947854</td>\n",
       "      <td>0.947564</td>\n",
       "      <td>0.947854</td>\n",
       "      <td>0.947406</td>\n",
       "      <td>0.966283</td>\n",
       "      <td>0.970772</td>\n",
       "      <td>0.961835</td>\n",
       "      <td>0.885001</td>\n",
       "      <td>0.871259</td>\n",
       "      <td>0.899184</td>\n",
       "      <td>{'0': {'precision': 0.9618346166722465, 'recall': 0.9707720898800473, 'f1-score': 0.9662826872950475, 'support': 5919.0}, '1': {'precision': 0.8991841491841492, 'recall': 0.8712591756070017, 'f1-score': 0.885001433897333, 'support': 1771.0}, 'accuracy': 0.9478543563068921, 'macro avg': {'precision': 0.9305093829281978, 'recall': 0.9210156327435245, 'f1-score': 0.9256420605961903, 'support': 7690.0}, 'weighted avg': {'precision': 0.9474062710387717, 'recall': 0.9478543563068921, 'f1-score': 0.947563688625691, 'support': 7690.0}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.297895</td>\n",
       "      <td>0.946944</td>\n",
       "      <td>0.946975</td>\n",
       "      <td>0.946944</td>\n",
       "      <td>0.947008</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.965028</td>\n",
       "      <td>0.966007</td>\n",
       "      <td>0.885006</td>\n",
       "      <td>0.886505</td>\n",
       "      <td>0.883512</td>\n",
       "      <td>{'0': {'precision': 0.9660071029934043, 'recall': 0.9650278763304613, 'f1-score': 0.9655172413793104, 'support': 5919.0}, '1': {'precision': 0.88351153629713, 'recall': 0.8865047995482778, 'f1-score': 0.8850056369785795, 'support': 1771.0}, 'accuracy': 0.9469440832249675, 'macro avg': {'precision': 0.9247593196452671, 'recall': 0.9257663379393695, 'f1-score': 0.9252614391789449, 'support': 7690.0}, 'weighted avg': {'precision': 0.947008449076746, 'recall': 0.9469440832249675, 'f1-score': 0.9469754921733684, 'support': 7690.0}}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando el modelo en el conjunto de prueba (test_dataset)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='481' max='481' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [481/481 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de la evaluación en el conjunto de prueba: {'eval_loss': 0.17746470868587494, 'eval_accuracy': 0.9421326397919376, 'eval_f1_overall': 0.9407651667020986, 'eval_recall_overall': 0.9421326397919376, 'eval_precision_overall': 0.941584774447451, 'eval_f1_class_0_no_toxic': 0.9630858564910826, 'eval_recall_class_0_no_toxic': 0.9807399898631526, 'eval_precision_class_0_no_toxic': 0.9460560625814863, 'eval_f1_class_1_toxic': 0.8661654135338346, 'eval_recall_class_1_toxic': 0.8130999435347261, 'eval_precision_class_1_toxic': 0.9266409266409267, 'eval_classification_report': {'0': {'precision': 0.9460560625814863, 'recall': 0.9807399898631526, 'f1-score': 0.9630858564910826, 'support': 5919.0}, '1': {'precision': 0.9266409266409267, 'recall': 0.8130999435347261, 'f1-score': 0.8661654135338346, 'support': 1771.0}, 'accuracy': 0.9421326397919376, 'macro avg': {'precision': 0.9363484946112065, 'recall': 0.8969199666989394, 'f1-score': 0.9146256350124586, 'support': 7690.0}, 'weighted avg': {'precision': 0.941584774447451, 'recall': 0.9421326397919376, 'f1-score': 0.9407651667020986, 'support': 7690.0}}, 'eval_runtime': 44.7825, 'eval_samples_per_second': 171.719, 'eval_steps_per_second': 10.741, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configurar los Argumentos de Entrenamiento (Reemplaza model.compile) ---\n",
    "# Usamos TrainingArguments de Hugging Face\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=IntervalStrategy.EPOCH,   \n",
    "    save_strategy=IntervalStrategy.EPOCH,   \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "\n",
    "# --- Crear una instancia del Trainer (Reemplaza model.fit) ---\n",
    "# Se crea el Trainer con el modelo, argumentos y datasets\n",
    "trainer = Trainer(\n",
    "    model=model,                         # El modelo AutoModelForSequenceClassification\n",
    "    args=training_args,                  # Los argumentos de entrenamiento definidos arriba\n",
    "    train_dataset=train_dataset,         # Tus datos de entrenamiento (Dataset de PyTorch)\n",
    "    eval_dataset=val_dataset,            # Tus datos de validación (Dataset de PyTorch)\n",
    "    compute_metrics=compute_metrics      \n",
    ")\n",
    "\n",
    "# --- Entrenar el modelo ---\n",
    "# Inicia el entrenamiento llamando a trainer.train()\n",
    "trainer.train()\n",
    "\n",
    "# --- Evaluar el modelo con el conjunto de prueba ---\n",
    "print(\"\\nEvaluando el modelo en el conjunto de prueba (test_dataset)...\")\n",
    "eval_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "# Los resultados de la evaluación están en un diccionario.\n",
    "print(f\"Resultados de la evaluación en el conjunto de prueba: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b890e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_cleaned'] = df['text_cleaned'].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a64e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Modelo para multi-clase ---\n",
    "X = df['text_cleaned']\n",
    "y = df['multi_label']\n",
    "\n",
    "# Dividir en Entrenamiento+Validación (80%) y Prueba (20%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Dividir el conjunto Entrenamiento+Validación en Entrenamiento (80% del 80% = 64%) y Validación (20% del 80% = 16%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3ac77ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Mapeo de Etiquetas ---\n",
      "Etiquetas a IDs: {'Acción/Juego': 0, 'Gravemente Tóxico': 1, 'Levemente Tóxico': 2, 'No Tóxico': 3}\n",
      "IDs a Etiquetas: {0: 'Acción/Juego', 1: 'Gravemente Tóxico', 2: 'Levemente Tóxico', 3: 'No Tóxico'}\n",
      "\n",
      "Número total de clases: 4\n"
     ]
    }
   ],
   "source": [
    "# 1. Mapear etiquetas de texto a IDs numéricos\n",
    "label_column_name = 'multi_label' # Tu columna de etiquetas se llama 'multi_label'\n",
    "unique_labels = sorted(y.unique())\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id_to_label = {i: label for i, label in enumerate(unique_labels)}\n",
    "\n",
    "print(\"--- Mapeo de Etiquetas ---\")\n",
    "print(\"Etiquetas a IDs:\", label_to_id)\n",
    "print(\"IDs a Etiquetas:\", id_to_label)\n",
    "\n",
    "# Convertir las etiquetas de tus conjuntos a IDs numéricos\n",
    "y_train_ids = y_train.map(label_to_id).values\n",
    "y_val_ids = y_val.map(label_to_id).values\n",
    "y_test_ids = y_test.map(label_to_id).values\n",
    "\n",
    "# Definir el número total de clases (necesario para el modelo)\n",
    "num_classes = len(unique_labels)\n",
    "print(f\"\\nNúmero total de clases: {num_classes}\")\n",
    "\n",
    "X_train_np = np.array(list(X_train)).reshape(-1, 1)\n",
    "\n",
    "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "X_train_resampled_np, y_train_resampled_ids = oversampler.fit_resample(\n",
    "    X_train_np, \n",
    "    y_train_ids\n",
    ")\n",
    "\n",
    "X_train = X_train_resampled_np.flatten()\n",
    "y_train_ids = y_train_resampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "624affcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Carga y Preparación del Tokenizer de Hugging Face ---\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e254337",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "train_encodings = tokenizer(\n",
    "    list(X_train),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_length,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "val_encodings = tokenizer(\n",
    "    list(X_val),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_length,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    list(X_test),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_length,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# --- CONVERTIR ETIQUETAS Y DATOS A PyTorch Dataset ---\n",
    "# Esta clase es estándar cuando se usa el Trainer.\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels_ids):\n",
    "        self.encodings = encodings\n",
    "        # Las etiquetas deben ser tensores de PyTorch.\n",
    "        self.labels = torch.tensor(labels_ids, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Aseguramos que los valores de encodings sean también tensores de PyTorch\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Crear los objetos Dataset para entrenamiento, validación y prueba\n",
    "train_dataset = CustomDataset(train_encodings, y_train_ids) \n",
    "val_dataset = CustomDataset(val_encodings, y_val_ids)    \n",
    "test_dataset = CustomDataset(test_encodings, y_test_ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6668ff55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Cargar el modelo pre-entrenado DistilBERT para clasificación ---\n",
    "num_labels = num_classes\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "# Mover el modelo a la GPU si está disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86e07bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82015' max='82015' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82015/82015 2:39:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Overall</th>\n",
       "      <th>Recall Overall</th>\n",
       "      <th>Precision Overall</th>\n",
       "      <th>F1 Class 0 Acción/juego</th>\n",
       "      <th>Recall Class 0 Acción/juego</th>\n",
       "      <th>Precision Class 0 Acción/juego</th>\n",
       "      <th>F1 Class 1 Gravemente Tóxico</th>\n",
       "      <th>Recall Class 1 Gravemente Tóxico</th>\n",
       "      <th>Precision Class 1 Gravemente Tóxico</th>\n",
       "      <th>F1 Class 2 Levemente Tóxico</th>\n",
       "      <th>Recall Class 2 Levemente Tóxico</th>\n",
       "      <th>Precision Class 2 Levemente Tóxico</th>\n",
       "      <th>F1 Class 3 No Tóxico</th>\n",
       "      <th>Recall Class 3 No Tóxico</th>\n",
       "      <th>Precision Class 3 No Tóxico</th>\n",
       "      <th>Classification Report Dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.577349</td>\n",
       "      <td>0.897269</td>\n",
       "      <td>0.899394</td>\n",
       "      <td>0.897269</td>\n",
       "      <td>0.903557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'Acción/Juego': {'precision': 0.6582064297800339, 'recall': 0.8644444444444445, 'f1-score': 0.7473583093179635, 'support': 450.0}, 'Gravemente Tóxico': {'precision': 0.8552631578947368, 'recall': 0.8673894912427023, 'f1-score': 0.8612836438923396, 'support': 1199.0}, 'Levemente Tóxico': {'precision': 0.7038917089678511, 'recall': 0.7272727272727273, 'f1-score': 0.7153912295786758, 'support': 572.0}, 'No Tóxico': {'precision': 0.9552154195011338, 'recall': 0.9243006034009874, 'f1-score': 0.9395037635907444, 'support': 5469.0}, 'accuracy': 0.8972691807542262, 'macro avg': {'precision': 0.7931441790359389, 'recall': 0.8458518165902154, 'f1-score': 0.8158842365949308, 'support': 7690.0}, 'weighted avg': {'precision': 0.90355690591653, 'recall': 0.8972691807542262, 'f1-score': 0.8993940435132358, 'support': 7690.0}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.664243</td>\n",
       "      <td>0.907022</td>\n",
       "      <td>0.907284</td>\n",
       "      <td>0.907022</td>\n",
       "      <td>0.909019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'Acción/Juego': {'precision': 0.706766917293233, 'recall': 0.8355555555555556, 'f1-score': 0.7657841140529531, 'support': 450.0}, 'Gravemente Tóxico': {'precision': 0.8412698412698413, 'recall': 0.8840700583819849, 'f1-score': 0.8621390809272061, 'support': 1199.0}, 'Levemente Tóxico': {'precision': 0.8089430894308943, 'recall': 0.6958041958041958, 'f1-score': 0.7481203007518797, 'support': 572.0}, 'No Tóxico': {'precision': 0.9509803921568627, 'recall': 0.9400255988297678, 'f1-score': 0.9454712643678161, 'support': 5469.0}, 'accuracy': 0.9070221066319896, 'macro avg': {'precision': 0.8269900600377078, 'recall': 0.8388638521428761, 'f1-score': 0.8303786900249637, 'support': 7690.0}, 'weighted avg': {'precision': 0.9090188380136343, 'recall': 0.9070221066319896, 'f1-score': 0.9072841048391692, 'support': 7690.0}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>0.747953</td>\n",
       "      <td>0.904421</td>\n",
       "      <td>0.904824</td>\n",
       "      <td>0.904421</td>\n",
       "      <td>0.905901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'Acción/Juego': {'precision': 0.7037037037037037, 'recall': 0.8022222222222222, 'f1-score': 0.7497403946002077, 'support': 450.0}, 'Gravemente Tóxico': {'precision': 0.8438511326860841, 'recall': 0.8698915763135947, 'f1-score': 0.8566735112936344, 'support': 1199.0}, 'Levemente Tóxico': {'precision': 0.7790476190476191, 'recall': 0.715034965034965, 'f1-score': 0.7456700091157703, 'support': 572.0}, 'No Tóxico': {'precision': 0.9494091580502215, 'recall': 0.9402084476138234, 'f1-score': 0.9447864033073037, 'support': 5469.0}, 'accuracy': 0.9044213263979194, 'macro avg': {'precision': 0.8190029033719072, 'recall': 0.8318393027961514, 'f1-score': 0.824217579579229, 'support': 7690.0}, 'weighted avg': {'precision': 0.9059009230467077, 'recall': 0.9044213263979194, 'f1-score': 0.9048237714581308, 'support': 7690.0}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.879613</td>\n",
       "      <td>0.905462</td>\n",
       "      <td>0.905562</td>\n",
       "      <td>0.905462</td>\n",
       "      <td>0.906266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'Acción/Juego': {'precision': 0.7190569744597249, 'recall': 0.8133333333333334, 'f1-score': 0.7632950990615224, 'support': 450.0}, 'Gravemente Tóxico': {'precision': 0.8539786710418376, 'recall': 0.8682235195996664, 'f1-score': 0.8610421836228288, 'support': 1199.0}, 'Levemente Tóxico': {'precision': 0.7677543186180422, 'recall': 0.6993006993006993, 'f1-score': 0.7319304666056725, 'support': 572.0}, 'No Tóxico': {'precision': 0.9476199228083073, 'recall': 0.9427683305906016, 'f1-score': 0.9451879010082493, 'support': 5469.0}, 'accuracy': 0.9054616384915475, 'macro avg': {'precision': 0.822102471731978, 'recall': 0.8309064707060752, 'f1-score': 0.8253639125745682, 'support': 7690.0}, 'weighted avg': {'precision': 0.9062659158874111, 'recall': 0.9054616384915475, 'f1-score': 0.9055616684335523, 'support': 7690.0}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.041100</td>\n",
       "      <td>0.910525</td>\n",
       "      <td>0.904941</td>\n",
       "      <td>0.904134</td>\n",
       "      <td>0.904941</td>\n",
       "      <td>0.904497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'Acción/Juego': {'precision': 0.7163265306122449, 'recall': 0.78, 'f1-score': 0.7468085106382979, 'support': 450.0}, 'Gravemente Tóxico': {'precision': 0.8637510513036165, 'recall': 0.8565471226021685, 'f1-score': 0.8601340033500837, 'support': 1199.0}, 'Levemente Tóxico': {'precision': 0.8189473684210526, 'recall': 0.6800699300699301, 'f1-score': 0.7430754536771729, 'support': 572.0}, 'No Tóxico': {'precision': 0.9378612716763006, 'recall': 0.9493508868166026, 'f1-score': 0.9435711040436165, 'support': 5469.0}, 'accuracy': 0.9049414824447334, 'macro avg': {'precision': 0.8342215555033037, 'recall': 0.8164919848721753, 'f1-score': 0.8233972679272927, 'support': 7690.0}, 'weighted avg': {'precision': 0.9044974822916874, 'recall': 0.9049414824447334, 'f1-score': 0.9041344638910099, 'support': 7690.0}}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando el modelo en el conjunto de prueba (test_dataset)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1923' max='1923' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1923/1923 00:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de la evaluación en el conjunto de prueba: {'eval_loss': 0.6021584868431091, 'eval_accuracy': 0.9174252275682705, 'eval_f1_overall': 0.9175157609422612, 'eval_recall_overall': 0.9174252275682705, 'eval_precision_overall': 0.9182924623444785, 'eval_f1_class_0_Acción/Juego': 0.0, 'eval_recall_class_0_Acción/Juego': 0.0, 'eval_precision_class_0_Acción/Juego': 0.0, 'eval_f1_class_1_Gravemente_Tóxico': 0.0, 'eval_recall_class_1_Gravemente_Tóxico': 0.0, 'eval_precision_class_1_Gravemente_Tóxico': 0.0, 'eval_f1_class_2_Levemente_Tóxico': 0.0, 'eval_recall_class_2_Levemente_Tóxico': 0.0, 'eval_precision_class_2_Levemente_Tóxico': 0.0, 'eval_f1_class_3_No_Tóxico': 0.0, 'eval_recall_class_3_No_Tóxico': 0.0, 'eval_precision_class_3_No_Tóxico': 0.0, 'eval_classification_report_dict': {'Acción/Juego': {'precision': 0.7392996108949417, 'recall': 0.8444444444444444, 'f1-score': 0.7883817427385892, 'support': 450.0}, 'Gravemente Tóxico': {'precision': 0.8733552631578947, 'recall': 0.8857381150959133, 'f1-score': 0.8795031055900621, 'support': 1199.0}, 'Levemente Tóxico': {'precision': 0.7903846153846154, 'recall': 0.7185314685314685, 'f1-score': 0.7527472527472527, 'support': 572.0}, 'No Tóxico': {'precision': 0.95625, 'recall': 0.9511793746571585, 'f1-score': 0.9537079475662297, 'support': 5469.0}, 'accuracy': 0.9174252275682705, 'macro avg': {'precision': 0.839822372359363, 'recall': 0.8499733506822462, 'f1-score': 0.8435850121605334, 'support': 7690.0}, 'weighted avg': {'precision': 0.9182924623444785, 'recall': 0.9174252275682705, 'f1-score': 0.9175157609422612, 'support': 7690.0}}, 'eval_runtime': 48.9286, 'eval_samples_per_second': 157.168, 'eval_steps_per_second': 39.302, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# --- Configurar los Argumentos de Entrenamiento (Reemplaza model.compile) ---\n",
    "# Usamos TrainingArguments de Hugging Face\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=IntervalStrategy.EPOCH,   \n",
    "    save_strategy=IntervalStrategy.EPOCH,   \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1_overall\",\n",
    "    greater_is_better=True,                  \n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "\n",
    "# --- Crear una instancia del Trainer (Reemplaza model.fit) ---\n",
    "# Se crea el Trainer con el modelo, argumentos y datasets\n",
    "trainer = Trainer(\n",
    "    model=model,                         # El modelo AutoModelForSequenceClassification\n",
    "    args=training_args,                  # Los argumentos de entrenamiento definidos arriba\n",
    "    train_dataset=train_dataset,         # Tus datos de entrenamiento (Dataset de PyTorch)\n",
    "    eval_dataset=val_dataset,            # Tus datos de validación (Dataset de PyTorch)\n",
    "    compute_metrics=lambda p: compute_metrics(p, id_to_label),\n",
    ")\n",
    "\n",
    "# --- Entrenar el modelo ---\n",
    "# Inicia el entrenamiento llamando a trainer.train()\n",
    "trainer.train()\n",
    "\n",
    "# --- Evaluar el modelo con el conjunto de prueba ---\n",
    "print(\"\\nEvaluando el modelo en el conjunto de prueba (test_dataset)...\")\n",
    "eval_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "# Los resultados de la evaluación están en un diccionario.\n",
    "print(f\"Resultados de la evaluación en el conjunto de prueba: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_dir = './models/modelo_toxicidad_guardado'\n",
    "#os.makedirs(output_dir, exist_ok=True) \n",
    "\n",
    "# Guarda el modelo\n",
    "#model.save_pretrained(output_dir)\n",
    "\n",
    "# Guarda el tokenizador\n",
    "#tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55240f0b",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#6B7A8F; font-weight:bold;\">Conclusiones Generales sobre el Rendimiento del Modelo</h3>\n",
    "\n",
    "<p>Tras el proceso de entrenamiento y evaluación, nuestro modelo de clasificación de toxicidad ha demostrado un rendimiento sólido y prometedor, alcanzando una <strong>precisión general del ~91.74%</strong>. Esto indica que el sistema es altamente efectivo en identificar y clasificar correctamente la inmensa mayoría de los mensajes de chat.</p>\n",
    "\n",
    "<h4 style=\"color:#2E4053;\">💪 Fortalezas del Modelo</h4>\n",
    "<ul>\n",
    "  <li><strong>Alta Fiabilidad en lo Crucial:</strong> El modelo brilla especialmente en las categorías más críticas:</li>\n",
    "  <ul>\n",
    "    <li><strong>Mensajes \"No Tóxicos\":</strong> F1-score de ~95.4%. Excepcional para preservar conversaciones saludables sin sobre-moderar.</li>\n",
    "    <li><strong>Mensajes \"Gravemente Tóxicos\":</strong> F1-score de ~87.9% y recall de ~88.6%. Detecta eficazmente la toxicidad más severa, clave para proteger a la comunidad.</li>\n",
    "  </ul>\n",
    "</ul>\n",
    "\n",
    "<h4 style=\"color:#B03A2E;\">⚠️ Áreas de Oportunidad</h4>\n",
    "<ul>\n",
    "  <li><strong>Manejo de la Toxicidad Sutil:</strong> F1-score de ~75.3%, con un recall de ~71.8%. El sarcasmo y la ironía siguen siendo un reto.</li>\n",
    "  <li><strong>Clasificación de \"Acción/Juego\":</strong> F1-score de ~78.8%, con precisión de ~73.9%. Podría haber confusiones con otras categorías.</li>\n",
    "</ul>\n",
    "\n",
    "<div style=\"background-color: #eaf6fd; border-left: 5px solid #3498db; padding: 15px; margin-top: 20px; border-radius: 5px;\">\n",
    "  <h4 style=\"color:#6B7A8F;\">🔎 Implicaciones para Empresas</h4>\n",
    "  <p>Los resultados posicionan a este modelo como una herramienta <strong>robusta y escalable</strong> para la moderación de contenido. Su precisión en detectar toxicidad grave y su fiabilidad con mensajes no tóxicos, lo hacen ideal para empresas que buscan proteger su marca y fomentar comunidades online más sanas y atractivas.</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
